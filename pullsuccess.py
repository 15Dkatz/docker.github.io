import urllib2
from bs4 import BeautifulSoup
from subprocess import call
import imghdr
import shutil
import base64

# docker run -it --rm --name pullsuccess -v "$PWD":/app -w /app docs/pullsuccess python pullsuccess.py

call(["rm", "-rf", "_kb"])
call(["mkdir", "_kb"])
call(["mkdir", "_kb/images"])
requrl = "https://success.docker.com/@api/deki/pages/"
response = urllib2.urlopen(requrl)
soup = BeautifulSoup(response, 'lxml')
keytags = ["component:ucp","component:dtr","component:hub","product:cloud","product:datacenter"]
keytitles = ["Universal Control Plane", "Docker Trusted Registry", "Docker Hub", "Docker Cloud", "Docker Datacenter"]
keyurls = ["/troubleshoot/universal-control-plane/","/troubleshoot/docker-trusted-registry/","/troubleshoot/docker-hub/","/troubleshoot/docker-cloud/","/troubleshoot/docker-datacenter/"]
for page in soup.find_all('page'):
    fileout = '';
    skipme = False
    if str(type(page.path.string)) == "<class 'bs4.element.NavigableString'>":
        if page.path.string.find("Internal") > -1:
            skipme = True
            print 'Skipping ' + page['id'] + ' because path=' + page.path.string
    if skipme == False:
        requrl = 'https://success.docker.com/@api/deki/pages/' + page['id']
        pagemetadata = urllib2.urlopen(requrl)
        metadatasoup = BeautifulSoup(pagemetadata, 'lxml')
        fileout += '---\n'
        fileout += 'title: \"' + page.title.string.replace('"','') + '\"\n'
        fileout += 'id: ' + page['id'] + '\n'
        fileout += 'draftstate: ' + page['draft.state'] + '\n'
        fileout += 'deleted: '  + page['deleted'] + '\n'
        fileout += 'layout: docs\n'
        fileout += 'permalink: /kb/' + page['id'] + '/\n'
        fileout += 'originalpath: ' + page.find('uri.ui').string + '\n'
        fileout += 'source: https://success.docker.com/@api/deki/pages/' + page['id'] + '/contents' + '\n'
        fileout += 'tags:' + '\n'
        for thistag in metadatasoup.find_all('tag'):
            fileout += '- tag: \"' + thistag['value'] + '\"' + '\n'
        fileout += '---' + '\n'
        fileout += '{% raw %}\n'

        requrl = 'https://success.docker.com/@api/deki/pages/' + page['id'] + '/contents'
        pagecontents = urllib2.urlopen(requrl)
        contentsoup = BeautifulSoup(pagecontents, 'html.parser')
        rawhtml = BeautifulSoup(contentsoup.get_text(), 'html.parser')
        # slim <h*> tag attributes to just an autogenerated ID, for right-nav
        headingList=['h1','h2','h3','h4','h5','h6']
        headingTypeIndex = 0
        for headingType in headingList:
            headerIndex = 0
            for header in rawhtml.find_all(headingType):
                header.attrs = {'id': str(headingTypeIndex) + '-' + str(headerIndex) }
                # print header
                headerIndex = headerIndex + 1
            headingTypeIndex = headingTypeIndex + 1
        # kill "No headers" <em> instances
        ems = rawhtml.find_all('em')
        for em in ems:
            if em.text.strip() == 'No headers':
                em.extract()
        # kill ending nav OL lists
        internals = rawhtml.find_all(attrs={"rel":"internal"})
        for thisin in internals:
            if thisin.name=='a':
                if thisin['href'][0]=="#":
                    if thisin.parent.parent.name=="ol":
                        thisin.parent.parent.extract()
        # rewrite links
        for link in rawhtml.find_all('a'):
            if link.has_attr('href'):
                link['href'] = link['href'].replace('https://docs.docker.com','')
                for thisPage in soup.find_all('page'):
                    if link['href'] in thisPage.find('uri.ui').string:
                        #print link['href'] + ' changed to: ' + '/kb/' +  thisPage['id'] + '/'
                        link['href'] = '/kb/' +  thisPage['id'] + '/'
        # save all images
        user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
        headers = { 'User-Agent' : user_agent }
        images = rawhtml.find_all('img')
        imageIndex = 0
        for img in images:
            imgRequest = urllib2.Request(img['src'], headers=headers)
            try:
                imgData = urllib2.urlopen(imgRequest).read()
            except urllib2.URLError, e:
                if e.args[0]=='unknown url type: data':
                    rawimgdata = img['src']
                    head, data = rawimgdata.split(',', 1)
                    file_ext = head.split(';')[0].split('/')[1]
                    imgData = base64.b64decode(data)
            newFileName = '_kb/images/' + page['id'] + '-' + str(imageIndex)
            newSrcName = '/kb/images/' + page['id'] + '-' + str(imageIndex)
            output = open(newFileName,'wb')
            output.write(imgData)
            output.close()
            newImageFileExt = imghdr.what(newFileName)
            shutil.move(newFileName, newFileName + '.' + newImageFileExt)
            img['src'] = newSrcName + '.' + newImageFileExt
            print 'Saved: ' + newFileName + '.' + newImageFileExt
            imageIndex = imageIndex + 1
        # Insert navigation
        for thistag in metadatasoup.find_all('tag'):
            for idx, val in enumerate(keytags):
                #print thistag['value'] + '(thistag["value"]) comparing to (val)' + val
                if thistag['value']==val:
                    fileout += '<a href="' + keyurls[idx] + '" class="button outline-btn">Back to: ' + keytitles[idx] + '</a>'
        fileout += rawhtml.prettify() + '\n'
        fileout += '{% endraw %}\n'
        f = open('_kb/' + page['id'] + '.html', 'w+')
        f.write(fileout.encode('utf8'))
        f.close
        print 'Success writing _kb/' + page['id'] + '.html'
